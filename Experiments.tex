\section{Experiments}
\label{sec:experiment}
%
\subsection{Datasets and Metric}

The test set is divided into three datasets: INA, DW and 3-24. The INA dataset contains a full week of broadcast for 2 TV channels for a total duration of 90 hours in French. The DW dataset~\cite{EUMSSI} is composed of video downloaded from Deutsche Welle website, in English and German for a total duration of 50 hours. The last dataset contains 13 hours of broadcast from 3/24 Catalan TV news channel. Each shot has been tagged with the names of people who appear and speak within that shot.

The task is evaluated indirectly as an information retrieval task, using the folllowing principle.
%
For each query $q \in \queries \subset \refNames$ (\texttt{first\-name\_lastname}), returned shots are first sorted by the edit distance between the hypothesized person name and the query $q$ and then by confidence scores.
Average precision $\text{AP}(q)$ is then computed classically based on the list of relevant shots (according to the groundtruth) and the sorted list of shots. Finally, Mean Average Precision is computed as follows:
\begin{align}
            \text{MAP} & = \frac{1}{|\queries|} \sum_{q \in \queries} \text{AP}(q) \nonumber
\end{align}

\subsection{Results}

\begin{compactitem}
  \item Sub. (2) used LIUM speaker diarization with our OCR-NER.
  \item Sub. (2) used OpenFace for face naming without talking score with our OCR-NER.
	\item Sub. (3) used our face naming without talking score with our OCR-NER.
	\item Sub. (4) used our face naming with talking score. 
	\item Sub. (5) used the combination of talking face naming in sub. (3) with speaker naming.
\end{compactitem}

\begin{table}[tb]
\centering
\begin{tabular}{c|c|c|c|}
\cline{2-4}
                                & MAP@1  & MAP@10 & MAP@100  \\ \hline
 \multicolumn{1}{|c|}{Sub. (1)} & 29.9   & 26.2   & 25.2 \\ \hline
 \multicolumn{1}{|c|}{Sub. (2)} & 65.8   & 46.0   & 45.0 \\ \hline
 \multicolumn{1}{|c|}{Sub. (3)} & 62.3   & 50.3   & 49.2 \\ \hline
 \multicolumn{1}{|c|}{Sub. (4)} & 69.3   & 57.0   & 55.8 \\ \hline
 \multicolumn{1}{|c|}{Sub. (5)} & 73.6   & 59.8   & 57.9 \\ \hline

\end{tabular}
\vspace*{-2mm}
\caption{Benchmarking results of our submissions. Details of each submission in the text.}
\vspace*{-2mm}
\label{tab:mediaeval}
\end{table}


%BEGINNING results IRISA/PUCMINAS---------------------------------------------------

\begin{table}[!t]
\caption{Mean average precisions @K obtained on the development and test sets using approaches by IRISA - PUC Minas. {\it no prop.}: no tag propagation. MST: tag propagation using the Minimum-Spanning Tree approach. RW: tag propagation using the Random Walk approach.}
\label{table_example}
\centering

\begin{tabular}{|c||c|c|c|c|}

\multicolumn{5}{ c }{Dev16)}\\
\hline
 & \multicolumn{4}{| c |}{MAP@K (\%)}\\
\hline
K  & 1 & 5 & 10 & 100\\
\hline
\hline
no prop.  & 81.4 & 66.3 & 64.2 & 61.8\\
\hline
MST & 81.4 & 71.1 & 69.5 & 67.1\\
\hline
RW & 81.4 & 71.2 & 69.7 & 67.1\\
\hline

\multicolumn{5}{ c }{Test16}\\
\hline
 & \multicolumn{4}{| c |}{MAP@K (\%)}\\
\hline
K  & 1 & 5 & 10& 100\\
\hline
\hline
no prop. & 55.9 & 35.8 & 33.8 & 32.8\\
\hline
MST & 68.9 & 57.5 & 55.4 & 53.6\\
\hline
RW & 71.3 & 59.7 & 57.4 & 55.5\\
\hline
\end{tabular}
\end{table}
%END results IRISA/PUCMINAS---------------------------------------------------

\endinput
